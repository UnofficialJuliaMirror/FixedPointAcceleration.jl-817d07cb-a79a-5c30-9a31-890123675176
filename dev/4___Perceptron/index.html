<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>- · FixedPointAcceleration</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>FixedPointAcceleration</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">FixedPointAcceleration.jl</a></li><li><a class="toctext" href="../1_FixedPoints/">1 Fixed point acceleration</a></li><li><a class="toctext" href="../2_Algorithms/">2 Acceleration algorithms</a></li><li><a class="toctext" href="../3_UsingAdvice/">3  Using the FixedPointAcceleration package</a></li><li><a class="toctext" href="../4_Applications/">4 Applications</a></li><li><a class="toctext" href="../4___EquilibriumPriceVector/">-</a></li><li class="current"><a class="toctext" href>-</a><ul class="internal"><li><a class="toctext" href="#.2-The-Perceptron-Classifier-1">4.2 The Perceptron Classifier</a></li></ul></li><li><a class="toctext" href="../4___ExpectationMaximisation/">-</a></li><li><a class="toctext" href="../4___ConsumptionSmoothing/">-</a></li><li><a class="toctext" href="../5_TerminationConditions/">5 Termination conditions and Error handling.</a></li><li><a class="toctext" href="../99_refs/">References</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>-</a></li></ul><a class="edit-page" href="https://github.com/s-baumann/FixedPointAcceleration.jl/blob/master/docs/src/4___Perceptron.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>-</span><a class="fa fa-bars" href="#"></a></div></header><h2><a class="nav-anchor" id=".2-The-Perceptron-Classifier-1" href="#.2-The-Perceptron-Classifier-1">4.2 The Perceptron Classifier</a></h2><p>The perceptron is one of the oldest and simplest machine learning algorithms (Rosenblatt 1958). In its simplest form, for each observation it is applied it uses an N-dimensional vector of features x together with N+1 weights w to classify the observation as being in category one or category zero. It classifies observation j as a type one if <span>$w_0 + \sum_{i=1}^N w_i x_{i,j}  &gt; 0$</span> and as a type zero otherwise.</p><p>The innovation of the perceptron was its method for training its weights, w. This is done by looping over a set of observations that can be used for training (the &quot;training set&quot;) and for which the true category information is available. The perceptron classifies each observation. When it correctly classifies an observation no action is taken. On the other hand when the perceptron makes an error then it updates its weights with the following expressions.</p><div>\[w_{0}^\prime = w_{0} + ( d_{j} - y_{j} )\]</div><div>\[w_{i}^\prime = w_{i} + ( d_{j} - y_{j} ) x_{j,i} \hspace{1cm} \text{ for } i \geq 0\]</div><p>Where <span>$w_i$</span> is the old weight for the <span>$i$</span>&#39;th feature and <span>$w_{i}^\prime$</span> is the updated weight. <span>$x_{j,i}$</span> is the feature value for observation <span>$j$</span>&#39;s feature <span>$i$</span>, <span>$d_{j}$</span> is the category label for observation <span>$j$</span> and <span>$y_j$</span> is the perceptron&#39;s prediction for this observation’s category.</p><p>This training algorithm can be rewritten as fixed point problem. We can write a function that takes perceptron weights, loops over the data updating these weights and then returns the updated weight vector. If the perceptron classifies every observation correctly then the weights will not update and we are at a fixed point.<a href="#footnote-7">[7]</a></p><p>Most acceleration algorithms perform poorly in accelerating the convergence of this perceptron training algorithm. This is due to the perceptron often converging by a ﬁxed increment. This occurs because multiple iterates can result in the same observations being misclassiﬁed and hence the same changeintheweights. Asaresultwewillusethesimplemethodwhichisguaranteedtobeconvergent for this problem (Novikoff, 1963).</p><p>First we generate a dataset:</p><pre><code class="language-none"># Generating linearly separable data
using Distributions
using FixedPointAcceleration
using Random
using DataFrames
nobs = 20
Random.seed!(1234)
data1 = DataFrame([rand(Normal(3,2), nobs), rand(Normal(8,2), nobs), repeat([-1.0],nobs)], [:x1, :x2, :y])
data2 = DataFrame([rand(Normal(-4,2), nobs), rand(Normal(10,12), nobs), repeat([1.0],nobs)], [:x1, :x2, :y])
data  = vcat(data1,data2)
# Plotting it
using Plots
plot(data1.x1, data1.x2,seriestype=:scatter)
plot!(data2.x1, data2.x2,seriestype=:scatter)</code></pre><p>Now we write a function that will take a set of weights, update them and return the updated weights.</p><pre><code class="language-none">function IteratePerceptronWeights(w, LearningRate = 1)
    for i in 1:length(data[:y])
        target = data[i,:y]
        score = w[1] + (w[2]*data[i,:x1]) + (w[3]*data[i,:x2])
        ypred = 2*((score &gt; 0)-0.5)
        if abs(target-ypred) &gt; 1e-10
            update = LearningRate * 0.5*(target-ypred)
            w[1] = w[1] + update
            w[2] = w[2] + update*data[i,:x1]
            w[3] = w[3] + update*data[i,:x2]
        end
    end
    return(w)
end
InitialGuess = [1.0, -2.0, 0.5]
FP = fixed_point(IteratePerceptronWeights, InitialGuess; Algorithm = Simple, PrintReports = true)</code></pre><p>Only the simple method is convergent here and it is relatively slow taking 1121 iterations. We can still get a beneﬁt from accelerators however if we can modify the training algorithm to give training increments that change depending on distance from the ﬁxed point. This can be done by updating the weights by an amount proportional to a concave function of the norm of <span>$w_0 + \sum_{i=1}^N w_i x_{i,j}$</span>. Note that the instances in which the weights are not updated stay the same and hence the modiﬁed training function will result in the same set of ﬁxed points as the basic function. This is done in the next piece of code where the MPE method is used. It can be seen that there is a substantial increase in speed with only 54 iterations required by the MPE method.</p><pre><code class="language-none">function IteratePerceptronWeights(w, LearningRate = 1)
    for i in 1:length(data[:y])
        target = data[i,:y]
        score = w[1] + (w[2]*data[i,:x1]) + (w[3]*data[i,:x2])
        ypred = 2*((score &gt; 0)-0.5)
        if abs(target-ypred) &gt; 1e-10
            update = LearningRate * -sign(score) * sqrt(abs(score))
            w[1] = w[1] + update
            w[2] = w[2] + update*data[i,:x1]
            w[3] = w[3] + update*data[i,:x2]
        end
    end
    return(w)
end
InitialGuess = [1.0, -2.0, 0.5]
FP = fixed_point(IteratePerceptronWeights, InitialGuess; Algorithm = MPE, PrintReports = true)</code></pre><p>We can verify that the set of weights represented by the fixed_point function does correctly seperate the data by plotting it:</p><pre><code class="language-none"># Plotting new seperation line
x1 = -6.0:0.1:6.0
w = FP.FixedPoint_
x2_on_sep_line = (-w[1] .- w[2] .* x1) ./ w[3]
plot!(x1,x2_on_sep_line, label =&quot;SeperationLine&quot;)</code></pre><div class="footnote" id="footnote-7"><a href="#footnote-7"><strong>[7]</strong></a><p>Note that for perceptrons there are always uncountably many such fixed points where the perceptron correctly classifies the entire training set and will not further update. On the other hand it is possible that the data is not linearly separable in which case there may be no fixed point and the weights will continue to update forever.</p></div><footer><hr/><a class="previous" href="../4___EquilibriumPriceVector/"><span class="direction">Previous</span><span class="title">-</span></a><a class="next" href="../4___ExpectationMaximisation/"><span class="direction">Next</span><span class="title">-</span></a></footer></article></body></html>
