<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>- · FixedPointAcceleration</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>FixedPointAcceleration</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">FixedPointAcceleration.jl</a></li><li><a class="toctext" href="../1_FixedPoints/">1 Fixed point acceleration</a></li><li><a class="toctext" href="../2_Algorithms/">2 Acceleration algorithms</a></li><li><a class="toctext" href="../3_UsingAdvice/">3  Using the FixedPointAcceleration package</a></li><li><a class="toctext" href="../4_Applications/">4 Applications</a></li><li><a class="toctext" href="../4___EquilibriumPriceVector/">-</a></li><li><a class="toctext" href="../4___Perceptron/">-</a></li><li class="current"><a class="toctext" href>-</a><ul class="internal"><li><a class="toctext" href="#.3-Expectation-Maximisation-1">4.3 Expectation Maximisation</a></li></ul></li><li><a class="toctext" href="../4___ConsumptionSmoothing/">-</a></li><li><a class="toctext" href="../5_TerminationConditions/">5 Termination conditions and Error handling.</a></li><li><a class="toctext" href="../99_refs/">References</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>-</a></li></ul><a class="edit-page" href="https://github.com/s-baumann/FixedPointAcceleration.jl/blob/master/docs/src/4___ExpectationMaximisation.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>-</span><a class="fa fa-bars" href="#"></a></div></header><h2><a class="nav-anchor" id=".3-Expectation-Maximisation-1" href="#.3-Expectation-Maximisation-1">4.3 Expectation Maximisation</a></h2><p>Consider we have a set of data which has come from two different multivariate normal distributions. There is a probability <span>$\tau$</span> that a datapoint comes from the first multivariate distribution.</p><pre><code class="language-none"># Generating data from two two-dimensional gaussian processes
using Distributions
using FixedPointAcceleration
using Random
using DataFrames
true_tau = 0.6
nobs_1 = 400
nobs_2 = convert(Int, round(nobs_1 * ((1-true_tau)/true_tau)))
Random.seed!(1234)
mu_1 = [0.0,8.0]
cov_1 = [2.0,0.5,2.0]
covar_1 = Symmetric([cov_1[1] cov_1[2]; cov_1[2] cov_1[3]])
md_1 = MultivariateNormal(mu_1,covar_1)
mu_2 = [-4.0,10.0]
cov_2 = [2.0,-0.75,12.0]
covar_2 = Symmetric([cov_2[1] cov_2[2]; cov_2[2] cov_2[3]])
md_2 = MultivariateNormal(mu_2,covar_2)

rands_from_1 = transpose(rand(md_1, nobs_1))
rands_from_2 = transpose(rand(md_2, nobs_2))
data1 = DataFrame([rands_from_1[:,1], rands_from_1[:,2]], [:x1, :x2])
data2 = DataFrame([rands_from_2[:,1], rands_from_2[:,2]], [:x1, :x2])
dd  = vcat(data1,data2)
# Plotting it:
plot(data1.x1, data1.x2,seriestype=:scatter)
plot!(data2.x1, data2.x2,seriestype=:scatter)</code></pre><p>Now we want to estimate the parameter <span>$\tau$</span>, the means (represented above by mu_1 and mu_2) and the covariance matrices (represented above by cov_1, cov_2) using only the realised datapoints in the DataFrame called dd. We will refer to these parameters as <span>$\theta$</span>.</p><p>If we knew from which distribution each datapoint came, the above task would be considerably easier. We could separate the data and for each use <a href="https://en.wikipedia.org/wiki/Estimation_of_covariance_matrices">standard techniques</a> to find the expected mean and covariance matrix. We do not know from which distribution each datapoint came from however. We could use a guess for <span>$\theta$</span> to estimate the probabilities of each datapoint coming from each distribution however (and call this vector of estimates by <span>$Z$</span>). Then we could choose maximum likelihood estimates of <span>$\theta$</span> using our estimates of <span>$Z$</span> in the likelihood expression. This is the EM approach. Note that it lends itself well to fixed point acceleration - We can write a function that given <span>$\theta$</span> creates estimated probabilities of source distribution for each datapoint (<span>$Z$</span>) and uses these in a maximum likelihood expression to improve the estimate of <span>$\theta$</span>.</p><p>In this multivariate gaussian case there are simple expressions to choose parameters to maximise the likelihood. These are recounted on the <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">wikipedia article on expectation maximisation</a> and are used below:</p><pre><code class="language-none">function z_estimate_given_theta(x::Array{Float64,1}, md_1::MultivariateNormal, md_2::MultivariateNormal, tau::Float64)
    pdf_1 = pdf(md_1, x)
    pdf_2 = pdf(md_2, x)
    return tau*pdf_1 / (tau*pdf_1 + (1-tau)*pdf_2)
end

function update_tau(Z::Array{Float64,1})
    return mean(Z)
end

function update_mu(dd::DataFrame, Z::Array{Float64,1})
    X = convert(Array{Float64,2}, dd[[:x1, :x2]])
    sum_Z = sum(Z)
    updated_mu = (transpose(Z) * X) ./sum_Z
    return vec(updated_mu)
end

function update_cov(dd::DataFrame, updated_mu::Array{Float64,1}, Z::Array{Float64,1})
    X_minus_mu = convert(Array{Float64,2}, dd[[:x1, :x2]]) .- transpose(updated_mu)
    sum_Z = sum(Z)
    updated_cov = (transpose(Z .* X_minus_mu) * X_minus_mu) ./sum_Z
    return [updated_cov[1,1], updated_cov[1,2], updated_cov[2,2]]
end

function update_theta(theta::Array{Float64,1}, dd::DataFrame)
    # We will use the convention that theta&#39;s 11 entries are (mu_1, cov_1, mu_2, cov_2, tau). First unpacking theta:
    mu_1    = theta[[1,2]]
    cov_1   = theta[[3,4,5]]
    covar_1 = Symmetric([cov_1[1] cov_1[2]; cov_1[2] cov_1[3]])
    md_1 = MultivariateNormal(mu_1,covar_1)
    mu_2    = theta[[6,7]]
    cov_2   = theta[[8,9,10]]
    covar_2 = Symmetric([cov_2[1] cov_2[2]; cov_2[2] cov_2[3]])
    md_2 = MultivariateNormal(mu_2,covar_2)
    tau     = theta[11]
    # Getting Z
    Z = Array{Float64,1}(undef,size(dd)[1])
    for i in 1:size(dd)[1]
        Z[i] = z_estimate_given_theta([dd[i,:x1], dd[i,:x2]], md_1, md_2, tau)
    end

    # Updating Tau
    updated_tau = update_tau(Z)
    # Updating mu1
    updated_mu_1 = update_mu(dd,Z)
    updated_mu_2 = update_mu(dd, 1 .- Z)
    # Updating Cov
    updated_cov_1 = update_cov(dd, updated_mu_1, Z)
    updated_cov_2 = update_cov(dd, updated_mu_2, 1 .- Z)
    # Returning theta
    updated_theta = vcat(updated_mu_1, updated_cov_1, updated_mu_2, updated_cov_2, updated_tau)
    return updated_theta
end</code></pre><p>Now we can come up with a choice for an initial guess based on eyeballing the plotted data. We can then put it into the fixed_point function to get ML estimates of these distributional parameters as well as <span>$\tau$</span>:</p><pre><code class="language-none">InitialGuess = [0.5, 7.5, 2.0, 0.0, 2.0, -5.0, 7.5, 2.0, 0.0, 10.0, 0.5]
fp_anderson = fixed_point(x -&gt; update_theta(x,dd), InitialGuess; Algorithm = Anderson, PrintReports = true)
fp_simple   = fixed_point(x -&gt; update_theta(x,dd), InitialGuess; Algorithm = Simple, PrintReports = true)</code></pre><p>We can see that the Anderson method only takes 15 iterations while the simple method takes 80. By checking the generated fixedpoint against the data generation process it can also be verified that the fixedpoint we find provides quite good estimates.</p><footer><hr/><a class="previous" href="../4___Perceptron/"><span class="direction">Previous</span><span class="title">-</span></a><a class="next" href="../4___ConsumptionSmoothing/"><span class="direction">Next</span><span class="title">-</span></a></footer></article></body></html>
